{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "\n",
    "comments = []\n",
    "usernames = []\n",
    "count = 0\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "    \n",
    "wordCount = defaultdict(int)\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "for i in comments:\n",
    "    r = ''.join([c for c in i.lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        # w = stemmer.stem(w)\n",
    "        wordCount[w] += 1\n",
    "\n",
    "labels = []\n",
    "from sets import Set\n",
    "vocab = Set()\n",
    "for i in range(600):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        vocab.add(w)\n",
    "numberpos = 0.0\n",
    "numberneg = 0.0\n",
    "for l in open('opt_dataset/labels_1-700.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "        numberpos += 1\n",
    "    else:\n",
    "        labels.append(0)\n",
    "        numberneg += 1\n",
    "posprob = numberpos/(numberpos + numberneg)\n",
    "negprob = 1-posprob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "\n",
    "pos_wordCount_mult = defaultdict(int)\n",
    "neg_wordCount_mult = defaultdict(int)\n",
    "for w in vocab:\n",
    "    pos_wordCount_mult[w] = 0\n",
    "    neg_wordCount_mult[w] = 0\n",
    "numberpos_mult = 0\n",
    "numberneg_mult = 0\n",
    "# calculating the probabilites\n",
    "for i in range(600):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    bag = r.split()\n",
    "    for w in bag:\n",
    "        if(labels[i] == 1):\n",
    "            pos_wordCount_mult[w] += 1\n",
    "            numberpos_mult += 1\n",
    "        else:\n",
    "                neg_wordCount_mult[w] += 1\n",
    "                numberneg_mult += 1\n",
    "                \n",
    "pos_wordprob_mult = defaultdict(float)\n",
    "neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "for w in vocab:\n",
    "    pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "    neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def predpos_mult(words):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        if(w in vocab):\n",
    "            sum += math.log(pos_wordprob_mult[w])\n",
    "    return (math.log(posprob) + sum)\n",
    "            \n",
    "def predneg_mult(words):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        if(w in vocab):\n",
    "            sum += math.log(neg_wordprob_mult[w])\n",
    "    return (math.log(negprob) + sum)\n",
    "            \n",
    "predictions_mult = []\n",
    "LLP = []\n",
    "LLN = []\n",
    "for i in range(len(comments)):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    x = predpos_mult(words)\n",
    "    y = predneg_mult(words)\n",
    "    LLP.append(x)\n",
    "    LLN.append(y)\n",
    "    if(x > y):\n",
    "        predictions_mult.append(1)\n",
    "    else:\n",
    "        predictions_mult.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\n",
      "iteration number: 2\n",
      "iteration number: 3\n",
      "iteration number: 4\n"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "num_iter = 0\n",
    "while(num_iter < 4):\n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0\n",
    "    numberneg_mult = 0\n",
    "    # calculating the probabilites\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            if(predictions_mult[i] == 1):\n",
    "                pos_wordCount_mult[w] += 1\n",
    "                numberpos_mult += 1\n",
    "            else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "\n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "\n",
    "    predictions_mult = []\n",
    "    LLP = []\n",
    "    LLN = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words)\n",
    "        y = predneg_mult(words)\n",
    "        LLP.append(x)\n",
    "        LLN.append(y)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    \n",
    "    num_iter += 1\n",
    "    print \"iteration number:\",num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see absolutely no reason for expanding the opportunities F-1 nonimmigrant students who merely make it much harder for United States young people who have such a difficult time finding work. Please do not allow additional visas or additional time for F-1 individuals to work in the U.S. True label: 0 Predicted label: 1\n",
      "As a US citizen, I personally think OPT program abuse is too prevailing to be acceptable for US public. However, I do believe there should be common ground as US should retain talented foreign students while preventing abuse Therefore I suggest the new OPT rule should be improved by these steps:  1) very clear and high enough prevailing wage requirement for employers to recruit students on OPT. 2) set up a black list of companies that abuse OPT and H1b, those in black list will be forever barred from using OPT or H1b program. 3)OPT program should only be available for students from top 50-100 US universities, the current provision \"accredited Universities\" is too vague. I also suggest Chinese students to comment about how to improve the OPT extension as above, in this case, you can at least win support from relatively reasonable people like me True label: 0 Predicted label: 1\n",
      "I would like to lodge my opposition to increasing amounts of F1 students and the offering of local jobs to them. I myself am a tech worker and it is getting harder to get any sort of raise or increase in standard of living. True label: 0 Predicted label: 1\n",
      "Test accuracy: 0.97 Train accuracy: 0.978333333333\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = 0.0\n",
    "test_accuracy = 0.0\n",
    "for i in range(700):\n",
    "    if(predictions_mult[i] == labels[i]):\n",
    "        if i<600:\n",
    "            train_accuracy += 1\n",
    "        else:\n",
    "            test_accuracy += 1\n",
    "    elif(i>=600  and i<700):\n",
    "        print comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions_mult[i]\n",
    "print \"Test accuracy:\",test_accuracy/100,\"Train accuracy:\", train_accuracy/600 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
