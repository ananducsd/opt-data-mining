{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userName': 'Roxanne Liu', 'recievedDate': '11/08/2015', 'docID': 'ICEB-2015-0002-12058', 'comment': 'It is very enssential for America to keep the international students. They are epquiped with skills and knowledge. The development of America relies on talents. Competition should not be the reason to ask them to leave.', 'postedDate': '11/11/2015'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a small test file for convenience\n",
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "x = open(\"opt_dataset/opt.json\")\n",
    "i = 0\n",
    "training = open(\"opt_dataset/small_opt.json\",'w')\n",
    "\n",
    "for l in x:\n",
    "    if (i>=0) and (i<5000):\n",
    "        training.write(l)\n",
    "        if(i==0):\n",
    "            print l\n",
    "    i += 1\n",
    "training.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forming the initial vocabulary on the training set of 600 samples\n",
    "#Training set: 1-600\n",
    "#Test set: 601-700\n",
    "#Not using stemming currently, need to modify later\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "\n",
    "comments = []\n",
    "usernames = []\n",
    "count = 0\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# extracting the vocab\n",
    "from sets import Set\n",
    "vocab = Set()\n",
    "for i in range(600):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        vocab.add(w)\n",
    "        \n",
    "# extracing all the known labels\n",
    "labels = []\n",
    "for l in open('opt_dataset/labels_1-700.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "\n",
    "pos_wordCount_mult = defaultdict(int)\n",
    "neg_wordCount_mult = defaultdict(int)\n",
    "for w in vocab:\n",
    "    pos_wordCount_mult[w] = 0\n",
    "    neg_wordCount_mult[w] = 0\n",
    "numberpos_mult = 0\n",
    "numberneg_mult = 0\n",
    "\n",
    "# calculating the probabilites\n",
    "for i in range(600):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    bag = r.split()\n",
    "    for w in bag:\n",
    "        if(labels[i] == 1):\n",
    "            pos_wordCount_mult[w] += 1\n",
    "            numberpos_mult += 1\n",
    "        else:\n",
    "                neg_wordCount_mult[w] += 1\n",
    "                numberneg_mult += 1\n",
    "                \n",
    "pos_wordprob_mult = defaultdict(float)\n",
    "neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "for w in vocab:\n",
    "    pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "    neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def predpos_mult(words):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        if(w in vocab):\n",
    "            sum += math.log(pos_wordprob_mult[w])\n",
    "    return (math.log(posprob) + sum)\n",
    "            \n",
    "def predneg_mult(words):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        if(w in vocab):\n",
    "            sum += math.log(neg_wordprob_mult[w])\n",
    "    return (math.log(negprob) + sum)\n",
    "            \n",
    "predictions_mult = []\n",
    "LLP = []\n",
    "LLN = []\n",
    "for i in range(len(comments)):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    x = predpos_mult(words)\n",
    "    y = predneg_mult(words)\n",
    "    LLP.append(x)\n",
    "    LLN.append(y)\n",
    "    if(x > y):\n",
    "        predictions_mult.append(1)\n",
    "    else:\n",
    "        predictions_mult.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\n",
      "iteration number: 2\n",
      "iteration number: 3\n",
      "iteration number: 4\n",
      "iteration number: 5\n",
      "iteration number: 6\n"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "# Refer to https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation\n",
    "num_iter = 0\n",
    "print \"Number of comments for semi-supervised estimation:\",len(comments)\n",
    "while(num_iter < 6):\n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0\n",
    "    numberneg_mult = 0\n",
    "    # calculating the probabilites\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            if(predictions_mult[i] == 1):\n",
    "                pos_wordCount_mult[w] += 1\n",
    "                numberpos_mult += 1\n",
    "            else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "\n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "\n",
    "    predictions_mult = []\n",
    "    LLP = []\n",
    "    LLN = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words)\n",
    "        y = predneg_mult(words)\n",
    "        LLP.append(x)\n",
    "        LLN.append(y)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    \n",
    "    num_iter += 1\n",
    "    print \"iteration number:\",num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see absolutely no reason for expanding the opportunities F-1 nonimmigrant students who merely make it much harder for United States young people who have such a difficult time finding work. Please do not allow additional visas or additional time for F-1 individuals to work in the U.S. True label: 0 Predicted label: 1 positive: -298.482601465 negative: -302.521579897\n",
      "While some people arguing that extending OPT will put more Americans out of work, the simple truth is that you cannot ask a farmer to do the job of a painter, and you cannot ask a painter to do the job of an engineer. To be honest, highly educated people with good personality will never have trouble finding a job, regardless of being an American or a foreign student. Those who has trouble finding a job is people who have limited skills or bad personalities. The way to put more Americans at work is to EDUCATE them, not by limiting those who are ALREADY educated. If I am the company manager, if I cannot find the workers with the required skills because you SENT them home, I will likely to move my company to their home country rather than hiring someone here who is incapable, sounds like creating more opportunities for other Americans? what a joke! True label: 1 Predicted label: 0 positive: -916.973148439 negative: -916.138754806\n",
      "As a US citizen, I personally think OPT program abuse is too prevailing to be acceptable for US public. However, I do believe there should be common ground as US should retain talented foreign students while preventing abuse Therefore I suggest the new OPT rule should be improved by these steps:  1) very clear and high enough prevailing wage requirement for employers to recruit students on OPT. 2) set up a black list of companies that abuse OPT and H1b, those in black list will be forever barred from using OPT or H1b program. 3)OPT program should only be available for students from top 50-100 US universities, the current provision \"accredited Universities\" is too vague. I also suggest Chinese students to comment about how to improve the OPT extension as above, in this case, you can at least win support from relatively reasonable people like me True label: 0 Predicted label: 1 positive: -883.463808056 negative: -911.489104222\n",
      "Test accuracy: 0.97 Train accuracy: 0.976666666667\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = 0.0\n",
    "test_accuracy = 0.0\n",
    "for i in range(700):\n",
    "    if(predictions_mult[i] == labels[i]):\n",
    "        if i<600:\n",
    "            train_accuracy += 1\n",
    "        else:\n",
    "            test_accuracy += 1\n",
    "    elif(i>=600  and i<700):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words)\n",
    "        y = predneg_mult(words)\n",
    "        print comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions_mult[i],\"positive:\",x,\"negative:\",y\n",
    "print \"Test accuracy:\",test_accuracy/100,\"Train accuracy:\", train_accuracy/600 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
