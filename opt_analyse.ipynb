{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userName': 'Roxanne Liu', 'recievedDate': '11/08/2015', 'docID': 'ICEB-2015-0002-12058', 'comment': 'It is very enssential for America to keep the international students. They are epquiped with skills and knowledge. The development of America relies on talents. Competition should not be the reason to ask them to leave.', 'postedDate': '11/11/2015'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# forming the initial vocabulary on the training set of 600 samples\n",
    "# Training set: \n",
    "# Test set: \n",
    "# Not using stemming currently, need to modify later\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sets import Set\n",
    "\n",
    "comments = []\n",
    "usernames = []\n",
    "stop_words = set(stopwords.words('english'))\n",
    "should_stem = 0\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(w):\n",
    "    if(should_stem == 0):\n",
    "        return w\n",
    "    else:\n",
    "        return stemmer.stem(w)\n",
    "    \n",
    "#creating a small test file for convenience\n",
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "x = open(\"opt_dataset/opt.json\")\n",
    "i = 0\n",
    "training = open(\"opt_dataset/small_opt.json\",'w')\n",
    "\n",
    "for l in x:\n",
    "    if (i>=0) and (i<3000):\n",
    "        training.write(l)\n",
    "        if(i==0):\n",
    "            print l\n",
    "    i += 1\n",
    "training.close()\n",
    "\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "\n",
    "        \n",
    "# extracing all the known labels\n",
    "labels = []\n",
    "for l in open('opt_dataset/labels_1-700.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "def fit_params(num_examples, data_labels):\n",
    "    train_no = num_examples\n",
    "    print \"entered fit_aprams\"\n",
    "    # extracting the vocab\n",
    "    vocab = Set()\n",
    "    for i in range(train_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for w in r.split():\n",
    "            if(w not in stop_words):\n",
    "                w = stem(w)\n",
    "                vocab.add(w)\n",
    "                \n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    \n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0.0\n",
    "    numberneg_mult = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            w = stem(w)\n",
    "            if(w in vocab):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult[w] += 1\n",
    "                    numberpos_mult += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "    \n",
    "    return vocab, pos_wordprob_mult,neg_wordprob_mult,posprob, negprob\n",
    "\n",
    "import math\n",
    "def predpos_mult(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "            \n",
    "def predneg_mult(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "\n",
    "def train_NB(num_examples,data_labels):\n",
    "    vocab, pp, nn, p, n = fit_params(num_examples,data_labels)\n",
    "    predictions_mult = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words,vocab,pp,p)\n",
    "        y = predneg_mult(words,vocab,nn,n)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    return predictions_mult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions for accuracy prediction...\n",
    "def print_accuracies(predictions, no_train,verbatim):\n",
    "    train_accuracy = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    rate = 0.0\n",
    "    neg_examples = 0.0\n",
    "    for i in range(700):\n",
    "        if(i>=no_train and labels[i] == 0):\n",
    "            neg_examples += 1\n",
    "        if(predictions[i] == labels[i]):\n",
    "            if(labels[i] == 0 and i>=no_train):\n",
    "                rate += 1\n",
    "            if i<no_train:\n",
    "                train_accuracy += 1\n",
    "            else:\n",
    "                test_accuracy += 1\n",
    "                \n",
    "        elif(i>=no_train  and i<700 and verbatim == 1):\n",
    "            r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "            words = r.split()\n",
    "            x = predpos_mult(words) # Likelihood of positive\n",
    "            y = predneg_mult(words) # Likelihood of negative\n",
    "            print comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions[i],\"positive:\",x,\"negative:\",y\n",
    "    print \"\\n\",\"Test accuracy:\",test_accuracy/(700-no_train),\"Train accuracy:\", train_accuracy/no_train, \"frac negatives identified:\",(rate + 0.0001)/(neg_examples+0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.88 Train accuracy: 0.992 frac negatives identified: 0.351353104451\n"
     ]
    }
   ],
   "source": [
    "predictions = train_NB(500,labels)\n",
    "print_accuracies(predictions,500,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.91 Train accuracy: 0.976 frac negatives identified: 0.513514828338\n",
      "iteration number: 1\n",
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.92 Train accuracy: 0.976 frac negatives identified: 0.567568736301\n",
      "iteration number: 2\n",
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.92 Train accuracy: 0.978 frac negatives identified: 0.567568736301\n",
      "iteration number: 3\n",
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.925 Train accuracy: 0.978 frac negatives identified: 0.594595690282\n",
      "iteration number: 4\n",
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.925 Train accuracy: 0.978 frac negatives identified: 0.594595690282\n",
      "iteration number: 5\n",
      "entered fit_aprams\n",
      "\n",
      "Test accuracy: 0.925 Train accuracy: 0.978 frac negatives identified: 0.594595690282\n",
      "iteration number: 6\n"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "# Refer to https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation\n",
    "# running the iterative process a fixed number of times.\n",
    "\n",
    "num_iter = 0\n",
    "\n",
    "while(num_iter < 6):\n",
    "    predictions = train_NB(len(comments),predictions)\n",
    "    print_accuracies(predictions,500,0)\n",
    "    num_iter += 1\n",
    "    print \"iteration number:\",num_iter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
