{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userName': 'Roxanne Liu', 'recievedDate': '11/08/2015', 'docID': 'ICEB-2015-0002-12058', 'comment': 'It is very enssential for America to keep the international students. They are epquiped with skills and knowledge. The development of America relies on talents. Competition should not be the reason to ask them to leave.', 'postedDate': '11/11/2015'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#creating a small test file for convenience\n",
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "x = open(\"opt_dataset/opt.json\")\n",
    "i = 0\n",
    "training = open(\"opt_dataset/small_opt.json\",'w')\n",
    "\n",
    "for l in x:\n",
    "    if (i>=0) and (i<5000):\n",
    "        training.write(l)\n",
    "        if(i==0):\n",
    "            print l\n",
    "    i += 1\n",
    "training.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#forming the initial vocabulary on the training set of 600 samples\n",
    "#Training set: 1-600\n",
    "#Test set: 601-700\n",
    "#Not using stemming currently, need to modify later\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "\n",
    "comments = []\n",
    "usernames = []\n",
    "count = 0\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "\n",
    "\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# extracting the vocab\n",
    "from sets import Set\n",
    "vocab = Set()\n",
    "for i in range(600):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        vocab.add(w)\n",
    "        \n",
    "# extracing all the known labels\n",
    "labels = []\n",
    "for l in open('opt_dataset/labels_1-700.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "\n",
    "pos_wordCount_mult = defaultdict(int)\n",
    "neg_wordCount_mult = defaultdict(int)\n",
    "for w in vocab:\n",
    "    pos_wordCount_mult[w] = 0\n",
    "    neg_wordCount_mult[w] = 0\n",
    "numberpos_mult = 0\n",
    "numberneg_mult = 0\n",
    "\n",
    "# calculating the probabilites\n",
    "for i in range(600):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    bag = r.split()\n",
    "    for w in bag:\n",
    "        if(labels[i] == 1):\n",
    "            pos_wordCount_mult[w] += 1\n",
    "            numberpos_mult += 1\n",
    "        else:\n",
    "                neg_wordCount_mult[w] += 1\n",
    "                numberneg_mult += 1\n",
    "                \n",
    "pos_wordprob_mult = defaultdict(float)\n",
    "neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "for w in vocab:\n",
    "    pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "    neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def predpos_mult(words):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        if(w in vocab):\n",
    "            sum += math.log(pos_wordprob_mult[w])\n",
    "    return (math.log(posprob) + sum)\n",
    "            \n",
    "def predneg_mult(words):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        if(w in vocab):\n",
    "            sum += math.log(neg_wordprob_mult[w])\n",
    "    return (math.log(negprob) + sum)\n",
    "            \n",
    "predictions_mult = []\n",
    "LLP = []\n",
    "LLN = []\n",
    "for i in range(len(comments)):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    words = r.split()\n",
    "    x = predpos_mult(words)\n",
    "    y = predneg_mult(words)\n",
    "    LLP.append(x)\n",
    "    LLN.append(y)\n",
    "    if(x > y):\n",
    "        predictions_mult.append(1)\n",
    "    else:\n",
    "        predictions_mult.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of comments for semi-supervised estimation: 5000\n",
      "iteration number: 1\n",
      "iteration number: 2\n",
      "iteration number: 3\n",
      "iteration number: 4\n",
      "iteration number:"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "# Refer to https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation\n",
    "num_iter = 0\n",
    "print \"Number of comments for semi-supervised estimation:\",len(comments)\n",
    "\n",
    "# changing vocabulary to include all of the comments...\n",
    "vocab = Set()\n",
    "for i in range(len(comments)):\n",
    "    r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "    for w in r.split():\n",
    "        vocab.add(w)\n",
    "        \n",
    "# running the iterative process a fixed number of times. \n",
    "\n",
    "while(num_iter < 6):\n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0\n",
    "    numberneg_mult = 0\n",
    "    # calculating the probabilites\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            if(predictions_mult[i] == 1):\n",
    "                pos_wordCount_mult[w] += 1\n",
    "                numberpos_mult += 1\n",
    "            else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "\n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "\n",
    "    predictions_mult = []\n",
    "    LLP = []\n",
    "    LLN = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words)\n",
    "        y = predneg_mult(words)\n",
    "        LLP.append(x)\n",
    "        LLN.append(y)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    \n",
    "    num_iter += 1\n",
    "    print \"iteration number:\",num_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see absolutely no reason for expanding the opportunities F-1 nonimmigrant students who merely make it much harder for United States young people who have such a difficult time finding work. Please do not allow additional visas or additional time for F-1 individuals to work in the U.S. True label: 0 Predicted label: 1 positive: -298.372833802 negative: -305.006583436\n",
      "Enough is Enough. What ever happened to WE THE PEOPLE? True label: 0 Predicted label: 1 positive: -61.6589986102 negative: -62.2897943683\n",
      "Please allow current non-immigrant students studying STEM or recent graduates to compete for jobs. There are enough talented workers looking for jobs. Please do not allow immigrants to take these jobs away from them. True label: 0 Predicted label: 1 positive: -211.729850267 negative: -212.910692857\n",
      "As a US citizen, I personally think OPT program abuse is too prevailing to be acceptable for US public. However, I do believe there should be common ground as US should retain talented foreign students while preventing abuse Therefore I suggest the new OPT rule should be improved by these steps:  1) very clear and high enough prevailing wage requirement for employers to recruit students on OPT. 2) set up a black list of companies that abuse OPT and H1b, those in black list will be forever barred from using OPT or H1b program. 3)OPT program should only be available for students from top 50-100 US universities, the current provision \"accredited Universities\" is too vague. I also suggest Chinese students to comment about how to improve the OPT extension as above, in this case, you can at least win support from relatively reasonable people like me True label: 0 Predicted label: 1 positive: -887.275347677 negative: -912.684884483\n",
      "I would like to lodge my opposition to increasing amounts of F1 students and the offering of local jobs to them. I myself am a tech worker and it is getting harder to get any sort of raise or increase in standard of living. True label: 0 Predicted label: 1 positive: -264.001325952 negative: -265.667012966\n",
      "Test accuracy: 0.95 Train accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "train_accuracy = 0.0\n",
    "test_accuracy = 0.0\n",
    "for i in range(700):\n",
    "    if(predictions_mult[i] == labels[i]):\n",
    "        if i<600:\n",
    "            train_accuracy += 1\n",
    "        else:\n",
    "            test_accuracy += 1\n",
    "    elif(i>=600  and i<700):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words) // Likelihood of positive\n",
    "        y = predneg_mult(words) // Likelihood of negative\n",
    "        print comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions_mult[i],\"positive:\",x,\"negative:\",y\n",
    "print \"\\n\",\"Test accuracy:\",test_accuracy/100,\"Train accuracy:\", train_accuracy/600 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
