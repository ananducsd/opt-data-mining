% This is "sig-alternate.tex" V2.1 April 2013
% This file should be compiled with V2.5 of "sig-alternate.cls" May 2012
%
% This example file demonstrates the use of the 'sig-alternate.cls'
% V2.5 LaTeX2e document class file. It is for those submitting
% articles to ACM Conference Proceedings WHO DO NOT WISH TO
% STRICTLY ADHERE TO THE SIGS (PUBS-BOARD-ENDORSED) STYLE.
% The 'sig-alternate.cls' file will produce a similar-looking,
% albeit, 'tighter' paper resulting in, invariably, fewer pages.
%
% ----------------------------------------------------------------------------------------------------------------
% This .tex file (and associated .cls V2.5) produces:
%       1) The Permission Statement
%       2) The Conference (location) Info information
%       3) The Copyright Line with ACM data
%       4) NO page numbers
%
% as against the acm_proc_article-sp.cls file which
% DOES NOT produce 1) thru' 3) above.
%
% Using 'sig-alternate.cls' you have control, however, from within
% the source .tex file, over both the CopyrightYear
% (defaulted to 200X) and the ACM Copyright Data
% (defaulted to X-XXXXX-XX-X/XX/XX).
% e.g.
% \CopyrightYear{2007} will cause 2007 to appear in the copyright line.
% \crdata{0-12345-67-8/90/12} will cause 0-12345-67-8/90/12 to appear in the copyright line.
%
% ---------------------------------------------------------------------------------------------------------------
% This .tex source is an example which *does* use
% the .bib file (from which the .bbl file % is produced).
% REMEMBER HOWEVER: After having produced the .bbl file,
% and prior to final submission, you *NEED* to 'insert'
% your .bbl file into your source .tex file so as to provide
% ONE 'self-contained' source file.
%
% ================= IF YOU HAVE QUESTIONS =======================
% Questions regarding the SIGS styles, SIGS policies and
% procedures, Conferences etc. should be sent to
% Adrienne Griscti (griscti@acm.org)
%
% Technical questions _only_ to
% Gerald Murray (murray@hq.acm.org)
% ===============================================================
%
% For tracking purposes - this is V2.0 - May 2012

\documentclass{sig-alternate-05-2015}
\usepackage{epstopdf}

\begin{document}

% Copyright
\setcopyright{acmcopyright}
%\setcopyright{acmlicensed}
%\setcopyright{rightsretained}
%\setcopyright{usgov}
%\setcopyright{usgovmixed}
%\setcopyright{cagov}
%\setcopyright{cagovmixed}


\title{Classification of Optional Pratical Training (OPT) comments using a Naive bayes classifier}

%
% You need the command \numberofauthors to handle the 'placement
% and alignment' of the authors beneath the title.
%
% For aesthetic reasons, we recommend 'three authors at a time'
% i.e. three 'name/affiliation blocks' be placed beneath the title.
%
% NOTE: You are NOT restricted in how many 'rows' of
% "name/affiliations" may appear. We just ask that you restrict
% the number of 'columns' to three.
%
% Because of the available 'opening page real-estate'
% we ask you to refrain from putting more than six authors
% (two rows with three columns) beneath the article title.
% More than six makes the first-page appear very cluttered indeed.
%
% Use the \alignauthor commands to handle the names
% and affiliations for an 'aesthetic maximum' of six authors.
% Add names, affiliations, addresses for
% the seventh etc. author(s) as the argument for the
% \additionalauthors command.
% These 'additional authors' will be output/set for you
% without further effort on your part as the last section in
% the body of your article BEFORE References or any Appendices.

\numberofauthors{4} %  in this sample file, there are a *total*
% of EIGHT authors. SIX appear on the 'first-page' (for formatting
% reasons) and the remaining two appear in the \additionalauthors section.
%
\author{
% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each line of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\alignauthor 
Anand\\ 
       \email{a3anand@ucsd.edu}
% 2nd. author
\alignauthor Sampath\\
       \email{svelaga@ucsd.edu}
% 3rd. author
\alignauthor 
Jorge Garza\\
       \email{jgarzagu@ucsd.edu}
\and  % use '\and' if you need 'another row' of author names
% 4th. author
\alignauthor 
Adithya\\
       \email{akaravad@ucsd.edu}
}

\maketitle
\begin{abstract}
This paper aims to classify the optional practical training comments using a naive bayes classifier. We demonstrate the effectiveness of the Naive bayes approach and further enhance its performance using a simplified form  of an expectation\
maximisation algorithm. We explore how sentiments change over time, and also provide preliminary results that help in understanding how sentiments vary with ethnicity.
\end{abstract}

\section{Introduction}
OPT is a scheme in which students with F-1 visas are permitted by the United States Citizenship and Immigration Services (USCIS) to work for at most one year on a student visa towards getting practical training to complement their field of studies. On April 2, 2008, the department of homeland security(DHS) announced an extension to the OPT which was passed by USCIS as an interim rule. This rule allows who graduate in a Science, Technology, Engineering or Math majors can get at OPT extension for upto 17 months. 
In August 2015, a US federal court gave its verdict on a lawsuit challenging the 17-month OPT STEM extension. The court has decided that the interim rule was deficient as it was not subjected to public notice, comments and opinions. The court vacated the 2008 rule allowing the 17-month extension. However, a stay was put in place until February 12, 2016. DHS will have until then in order to take action regarding the fate of the STEM extension program. This rule was open to public comments for a one month duration, ending on Nov 18th. The comments are publicly available at \url{http://www.regulations.gov/#!docketDetail;D=ICEB-2015-0002}

\section{Information on the Data}
\subsection{Data collection}

\subsection{Data visualisation/Exploratory analysis}

\subsection{Dataset Labeling}
Since the original dataset is unlabeled, we manually labeled the first 900 comments as \textit{support} or \textit{oppose}. Out of these, the first 600 were used for training, comments from 601-700 constituted the validation set and 700-900 were used for testing .\ 
We used validation set to pick the best possible model from amongst a pool of possible models. 
\subsection{Predictive task}
Our main goal here was to classify whether a given comment is  supporting or an opposing. In addition, based on the classifier we obtained, we also examined how the proportions of supporting and opposing reviews varied with time. Finally, we tried to examine the trends on an ethnicity basis. The main idea of this was to check if the pattern folows the hypothesis that most Americans oppose OPT extension, while people from other ethnicities support it.
\subsection{Dataset Preprocessing}
As a preprocessing step, we removed all the punctuations from the words. We also changed all words to lower case letters, although a more rigorous model could make use of the caps information to identify stronger sentiments. Finally, all the common stop words were removed as they convey little meaning. 
\section{Related Literature}

\section{Algorithms and models tried for classification}
Broadly speaking, there are two classes of algorithms that could be tried to classify the comment labels - supervised and unsupervised.For unsupervised learning, we tried clustering based on the \textit{tf-idf} features extracted from the text with the Eucledian distance metric. \
. Hierarchical clustering runs in time $O (n^3)d$, where $n$ is the number of datapoints and $d$ is the number of dimensions of the feature vector, making it very slow for large datasets. Therefore,  we implemented K-means clustering which is much faster. However, no useful clusters were identified and the accuracies were no better than those of a random classifier. This is to be expected because there is no coherent structure across the different comments - they are of varying lengths and contain different kinds of vocabulary to express the same sentiment, thus rendering Eucledian distance as a very bad distance measure.

Naive bayes performs particularly well for text classification despite the aggressive assumption it makes about independence.  The reason for this is thought to be because, although naive bayes fails to produce good estimates of the probabilities, we do not require the absolute values of these, but only the relative ordering to estimate the MAP estimate. Reports by cite Vikesh suggested that Naive Bayes indeed performs well on this dataset. There are at least two popular versions of Naive Bayes - Multinomial and Bernoulli. 
Bernoulli naive bayes makes the assumption that each document belonging to a class contains osccurences of some words that are described by the probability distribution of the words belonging to that class. The  Probability of the document given the class can then be modeled by:
\begin{align*}
P(doc|class) = \prod_{w \in doc}P(w|class) \prod_{w\not\in doc}(1-P(w|class)
\end{align*}
On the other hand, Multinomail naive bayes assumes that the document of a particular class is generated by the following generative process - First, the length is chosen according to some distribution(which we don't care, as the length does not depend on the class labels). Then, every word in the document is generated by a multinomail distribution over the words belonging to that class. In this case, the corresponding probability can be modeled by:

\begin{align*}
P(doc|class) = P(|length(doc)|)\prod_{w \in doc}P(w|class)
\end{align*}

We implemented both multinomail and bernoulli naive bayes, but we considered only the multinomial model for further analysis because the run time was better whilst the performance was similar.

Multinomial models using just unigrams, just bigrams, and using both unigrams and bigrams were considered. Intial results showing the accuracies on the training set, test set and validation set are presented below. 
INSERT TABLE HERE
From the table, it is clear that:
\begin{enumerate}
\item The bigram only model overfits to the training data
\item The unigram+bigram model is performing almost the same as the unigram only model
\end{enumerate}
Based on these, and the obvious speed of unigram only over unigram+bigram, we considered the unigram only model for all further experiments. 
\subsection{Semi supervised estimation}
It has been suggested by the authors in CITE, that in cases where the number of training examples is small, the performance of the naive bayes classifier can be improved by combining it with an expectation maximisation algorithm. In short, the authors suggest to do this :
\begin{enumerate}
\item Predict the class probabilites $P(class|data)$ for all examples in the dataset
\item Retrain the model based on \textit{class probabilities} estimated in the previous step
\end{enumerate}
The first step above is an expectation step  in disguise, and the second step corresponds to the maximisation. Although the second step requires us to retrain the model based on the probabilities in the previous step, we relax this step as follows:
Relaxed expectation maximisation:

\begin{enumerate}
\item Predict the class probabilites $P(class|data)$ for all examples in the dataset
\item Retrain the model based on \textit{class labels} estimated in the previous step
\end{enumerate}
This algorithm, which we'll refer to as  "classification maximisation" algorithm is a convenient approximation to the more rigorous expectation maximisation. What this means, is that we use the predicted labels as the actual labels and retrain the model based on these labels until convergence. These iterations significantly improve the accuracy of the naive bayes model by incorporating the knowledge from the large pool of unlabeled examples. Refer to FIGURE to see how the accuracy of the model changes with the iteration of the EM algorithm. 



\section{Results and discusision}

\section{Conclusions}
This paragraph will end the body of this sample document.
Remember that you might still have Acknowledgments or
Appendices; brief samples of these
follow.  There is still the Bibliography to deal with; and
we will make a disclaimer about that here: with the exception
of the reference to the \LaTeX\ book, the citations in
this paper are to articles which have nothing to
do with the present subject and are used as
examples only.
%\end{document}  % This is where a 'short' article might terminate

\section{Acknowledgments}
This section is optional; it is a location for you
to acknowledge grants, funding, editing assistance and
what have you.  In the present case, for example, the
authors would like to thank Gerald Murray of ACM for
his help in codifying this \textit{Author's Guide}
and the \textbf{.cls} and \textbf{.tex} files that it describes.


\section{References}
Generated by bibtex from your ~.bib file.  Run latex,
then bibtex, then latex twice (to resolve references)
to create the ~.bbl file.  Insert that ~.bbl file into
the .tex source file and comment out
the command \texttt{{\char'134}thebibliography}.
% This next section command marks the start of
% Appendix B, and does not continue the present hierarchy
\section{More Help for the Hardy}
% That's all folks!
\end{document}
