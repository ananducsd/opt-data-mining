{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userName': 'Roxanne Liu', 'recievedDate': '11/08/2015', 'docID': 'ICEB-2015-0002-12058', 'comment': 'It is very enssential for America to keep the international students. They are epquiped with skills and knowledge. The development of America relies on talents. Competition should not be the reason to ask them to leave.', 'postedDate': '11/11/2015'}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/2.7/site-packages/ipykernel/__main__.py:7: DeprecationWarning: the sets module is deprecated\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sets import Set\n",
    "\n",
    "\n",
    "comments = []\n",
    "usernames = []\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "# Alternatively, just remove the basic stop words...\n",
    "stop_words = {\"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\"\n",
    "             ,\"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"where\",\"will\",\"with\"}\n",
    "should_stem = 0\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(w):\n",
    "    if(should_stem == 0):\n",
    "        return w\n",
    "    else:\n",
    "        return stemmer.stem(w)\n",
    "    \n",
    "#creating a small test file for convenience\n",
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "x = open(\"opt_dataset/opt.json\")\n",
    "i = 0\n",
    "training = open(\"opt_dataset/small_opt.json\",'w')\n",
    "\n",
    "for l in x:\n",
    "    if (i>=0) and (i<30000):\n",
    "        training.write(l)\n",
    "        if(i==0):\n",
    "            print l\n",
    "    i += 1\n",
    "training.close()\n",
    "\n",
    "# forming the initial vocabulary on the training set of 600 samples\n",
    "# Training set: \n",
    "# Test set: \n",
    "# Not using stemming currently, need to modify later\n",
    "\n",
    "\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "\n",
    "def bigrams(words):\n",
    "    if(should_stem == 0):\n",
    "        return nltk.bigrams(words)\n",
    "    else:\n",
    "        for i in range(len(words)):\n",
    "            words[i] = stem(words[i])\n",
    "        return nltk.bigrams(words)\n",
    "\n",
    "        \n",
    "# extracing all the known labels\n",
    "labels = []\n",
    "for l in open('opt_dataset/labels_1-900.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "def fit_params(num_examples, data_labels,update_vocab):\n",
    "    train_no = num_examples\n",
    "    # extracting the vocab\n",
    "    if(update_vocab == 1):\n",
    "        vocab_no = train_no\n",
    "    else:\n",
    "        vocab_no = 700\n",
    "        \n",
    "    vocab = Set()\n",
    "    for i in range(vocab_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for w in r.split():\n",
    "            if(w not in stop_words):\n",
    "                w = stem(w)\n",
    "                vocab.add(w)\n",
    "\n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    \n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0.0\n",
    "    numberneg_mult = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            w = stem(w)\n",
    "            if(w in vocab):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult[w] += 1\n",
    "                    numberpos_mult += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "    \n",
    "    return vocab, pos_wordprob_mult,neg_wordprob_mult,posprob, negprob\n",
    "\n",
    "import math\n",
    "def predpos_mult(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "            \n",
    "def predneg_mult(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes bigrams!\n",
    "def fit_params_bi(num_examples, data_labels,update_vocab):\n",
    "    train_no = num_examples\n",
    "    # extracting the bigrams\n",
    "    if(update_vocab == 1):\n",
    "        vocab_no = train_no\n",
    "    else:\n",
    "        vocab_no = 700\n",
    "\n",
    "    all_bi = defaultdict(int)\n",
    "    for i in range(vocab_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for pair in bigrams(r.split()):\n",
    "            all_bi[pair]=1\n",
    "                \n",
    "    pos_wordCount_mult_bi = defaultdict(int)\n",
    "    neg_wordCount_mult_bi = defaultdict(int)\n",
    "    \n",
    "    for pair in all_bi:\n",
    "        pos_wordCount_mult_bi[pair] = 0\n",
    "        neg_wordCount_mult_bi[pair] = 0\n",
    "    numberpos_mult_bi = 0.0\n",
    "    numberneg_mult_bi = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for pair in bigrams(bag):\n",
    "            if(pair in all_bi):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult_bi[pair] += 1\n",
    "                    numberpos_mult_bi += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult_bi[pair] += 1\n",
    "                    numberneg_mult_bi += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult_bi = defaultdict(float)\n",
    "    neg_wordprob_mult_bi = defaultdict(float)\n",
    "\n",
    "    for pair in all_bi:\n",
    "        pos_wordprob_mult_bi[pair] = 1.0*(pos_wordCount_mult_bi[pair] + 1)/(len(all_bi) + numberpos_mult_bi)\n",
    "        neg_wordprob_mult_bi[pair] = 1.0*(neg_wordCount_mult_bi[pair] + 1)/(len(all_bi) + numberneg_mult_bi)\n",
    "    \n",
    "    return all_bi, pos_wordprob_mult_bi,neg_wordprob_mult_bi,posprob, negprob\n",
    "\n",
    "\n",
    "import math\n",
    "def predpos_mult_bi(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for pair in bigrams(words):\n",
    "        if(pair in vocab):\n",
    "            sum += math.log(prob[pair])\n",
    "    return (sum)\n",
    "            \n",
    "def predneg_mult_bi(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for pair in bigrams(words):\n",
    "        if(pair in vocab):\n",
    "            sum += math.log(prob[pair])\n",
    "    return (sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_NB(num_examples,data_labels,update_vocab):\n",
    "    vocab, pp, nn, p, n = fit_params(num_examples,data_labels,update_vocab)\n",
    "    all_bi, pp_bi, nn_bi, p, n = fit_params_bi(num_examples,data_labels,update_vocab)\n",
    "    predictions_mult = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words,vocab,pp,p) #+ predpos_mult_bi(words,all_bi,pp_bi,p)\n",
    "        y = predneg_mult(words,vocab,nn,n) #+ predneg_mult_bi(words,all_bi,nn_bi,n)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    return predictions_mult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions for accuracy prediction...\n",
    "def print_accuracies(predictions, no_train,verbatim):\n",
    "    train_accuracy = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    rate = 0.0\n",
    "    neg_examples = 0.0\n",
    "    for i in range(700):\n",
    "        if(i>=no_train and labels[i] == 0):\n",
    "            neg_examples += 1\n",
    "        if(predictions[i] == labels[i]):\n",
    "            if(labels[i] == 0 and i>=no_train):\n",
    "                rate += 1\n",
    "            if i<no_train:\n",
    "                train_accuracy += 1\n",
    "            else:\n",
    "                test_accuracy += 1\n",
    "                \n",
    "        elif(i>=no_train  and i<700 and verbatim == 1):\n",
    "            print comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions[i]\n",
    "    print \"\\n\",\"Test accuracy:\",test_accuracy/(700-no_train),\"Train accuracy:\", train_accuracy/no_train, \"frac negatives identified:\",(rate + 0.0001)/(neg_examples+0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.86 Train accuracy: 0.98 frac negatives identified: 0.333336507921\n"
     ]
    }
   ],
   "source": [
    "predictions = train_NB(600,labels,1)\n",
    "print_accuracies(predictions,600,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\n",
      "\n",
      "Test accuracy: 0.943333333333 Train accuracy: 0.976666666667 frac negatives identified: 0.729167230902\n",
      "iteration number: 2\n",
      "\n",
      "Test accuracy: 0.966666666667 Train accuracy: 0.981666666667 frac negatives identified: 0.895833550347\n",
      "iteration number: 3\n",
      "\n",
      "Test accuracy: 0.963333333333 Train accuracy: 0.973333333333 frac negatives identified: 0.916666840277\n",
      "iteration number: 4\n",
      "\n",
      "Test accuracy: 0.963333333333 Train accuracy: 0.973333333333 frac negatives identified: 0.916666840277\n",
      "iteration number: 5\n",
      "\n",
      "Test accuracy: 0.96 Train accuracy: 0.973333333333 frac negatives identified: 0.916666840277\n",
      "iteration number: 6\n",
      "\n",
      "Test accuracy: 0.96 Train accuracy: 0.971666666667 frac negatives identified: 0.916666840277\n",
      "iteration number: 7\n",
      "\n",
      "Test accuracy: 0.96 Train accuracy: 0.971666666667 frac negatives identified: 0.916666840277\n",
      "iteration number: 8\n",
      "\n",
      "Test accuracy: 0.956666666667 Train accuracy: 0.971666666667 frac negatives identified: 0.916666840277\n",
      "iteration number: 9\n",
      "\n",
      "Test accuracy: 0.953333333333 Train accuracy: 0.971666666667 frac negatives identified: 0.916666840277\n"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "# Refer to https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation\n",
    "# running the iterative process a fixed number of times.\n",
    "\n",
    "num_iter = 1\n",
    "\n",
    "while(num_iter < 10):\n",
    "    print \"iteration number:\",num_iter\n",
    "    predictions = train_NB(len(comments),predictions,0)\n",
    "    print_accuracies(predictions,600,0)\n",
    "    num_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see absolutely no reason for expanding the opportunities F-1 nonimmigrant students who merely make it much harder for United States young people who have such a difficult time finding work. Please do not allow additional visas or additional time for F-1 individuals to work in the U.S. True label: 0 Predicted label: 1\n",
      "While some people arguing that extending OPT will put more Americans out of work, the simple truth is that you cannot ask a farmer to do the job of a painter, and you cannot ask a painter to do the job of an engineer. To be honest, highly educated people with good personality will never have trouble finding a job, regardless of being an American or a foreign student. Those who has trouble finding a job is people who have limited skills or bad personalities. The way to put more Americans at work is to EDUCATE them, not by limiting those who are ALREADY educated. If I am the company manager, if I cannot find the workers with the required skills because you SENT them home, I will likely to move my company to their home country rather than hiring someone here who is incapable, sounds like creating more opportunities for other Americans? what a joke! True label: 1 Predicted label: 0\n",
      "As a US citizen, I personally think OPT program abuse is too prevailing to be acceptable for US public. However, I do believe there should be common ground as US should retain talented foreign students while preventing abuse Therefore I suggest the new OPT rule should be improved by these steps:  1) very clear and high enough prevailing wage requirement for employers to recruit students on OPT. 2) set up a black list of companies that abuse OPT and H1b, those in black list will be forever barred from using OPT or H1b program. 3)OPT program should only be available for students from top 50-100 US universities, the current provision \"accredited Universities\" is too vague. I also suggest Chinese students to comment about how to improve the OPT extension as above, in this case, you can at least win support from relatively reasonable people like me True label: 0 Predicted label: 1\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n"
     ]
    }
   ],
   "source": [
    "print_accuracies(predictions,600,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# doing time analysis\n",
    "\n",
    "pos_date_count = defaultdict(int)\n",
    "neg_date_count = defaultdict(int)\n",
    "    \n",
    "def predictAllComments(num_examples,data_labels,update_vocab):\n",
    "    \n",
    "    vocab, pp, nn, p, n = fit_params(num_examples,data_labels,update_vocab)\n",
    "    all_bi, pp_bi, nn_bi, p, n = fit_params_bi(num_examples,data_labels,update_vocab)\n",
    "    predictions_mult = []\n",
    "    \n",
    "    print 'starting'\n",
    "    count = 0\n",
    "    \n",
    "    for i in read('opt_dataset/opt.json'):\n",
    "        #count += 1\n",
    "        #if (count % 1000 == 0): print count\n",
    "        comment = i['comment']\n",
    "        date = i['recievedDate']\n",
    "        r = ''.join([c for c in comment.lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words,vocab,pp,p) #+ predpos_mult_bi(words,all_bi,pp_bi,p)\n",
    "        y = predneg_mult(words,vocab,nn,n) #+ predneg_mult_bi(words,all_bi,nn_bi,n)\n",
    "        if(x > y):\n",
    "            pos_date_count[date] += 1\n",
    "        else:\n",
    "            neg_date_count[date] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<string>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<string>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    {'userName': 'Miao Yu', 'comment': 'I think the new rule is both important and necessary to both STEM students and US. For me, extended OPT could help me gain more practice on the subjects I leaned. And this practice is hard to gain either in university or short-time due to the complexity of engineering. For United States, engineering is the nature of the most of industries, which highly impact the economy. The extended OPT will enable STEM students use the knowledge they leant to help the growth of the economy. I strongly support this proposal. I think it is an important part of STEM education and OPT extension could make more student receive it. And this part of well educated students could bring benefits to industries. And it won't impair the job opportunities of natives since a company must pay more and go through a lot of processes to hire an international student.', 'docID': 'ICEB-2015-0002-0377', 'number': 10292, 'recievedDate': '10/19/2015', 'postedDate': '10/19/2015'}\u001b[0m\n\u001b[0m                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "predictAllComments(600,labels,1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Graph plot for time analysis\n",
    "from matplotlib.dates import drange, num2date\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import  datetime\n",
    "from datetime import timedelta\n",
    "import numpy as np\n",
    "import matplotlib.dates as mdates\n",
    "\n",
    "pos_dict = defaultdict(int)\n",
    "neg_dict = defaultdict(int)\n",
    "\n",
    "for key in pos_date_count:\n",
    "    d = datetime.strptime(key, '%m/%d/%Y').date()\n",
    "    pos_dict[d] = pos_date_count[key]\n",
    "    \n",
    "for key in neg_date_count:\n",
    "    d = datetime.strptime(key, '%m/%d/%Y').date()\n",
    "    neg_dict[d] = neg_date_count[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-16\n",
      "2015-11-23\n"
     ]
    }
   ],
   "source": [
    "list_of_dates = list(pos_dict.keys())\n",
    "\n",
    "print(min(list_of_dates))\n",
    "print(max(list_of_dates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dates = num2date(drange(min(list_of_dates), max(list_of_dates) + timedelta(1), timedelta(1)))\n",
    "\n",
    "pos_counts = np.asarray([pos_dict.get(d.date(), 0) for d in dates])\n",
    "neg_counts = np.asarray([neg_dict.get(d.date(), 0) for d in dates])\n",
    "\n",
    "frac = []\n",
    "for i in range(len(pos_counts)):\n",
    "    tot = pos_counts[i] + neg_counts[i]\n",
    "    if(tot == 0):\n",
    "        frac.append(0)\n",
    "    else:\n",
    "        val = pos_counts[i] * 1.0 /tot\n",
    "        frac.append(val)\n",
    "#print dates\n",
    "#print pos_counts\n",
    "#print neg_counts\n",
    "#step(dates, pos_counts)\n",
    "#step(dates, neg_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0, 0, 1.0, 0.99809523809523815, 0.99195710455764075, 0.99715909090909094, 1.0, 1.0, 0.9876160990712074, 0.97041420118343191, 0.96842105263157896, 0.97468354430379744, 0.96296296296296291, 0.62487852283770651, 0, 0, 0.63244047619047616, 0.78174603174603174, 0.93805309734513276, 0.8177905308464849, 0.60151324085750313, 0, 0, 0.65180102915951976, 0.73307729236677865, 0.95958624007697857, 0.97975010771219306, 0.98942682248191427, 0.98011363636363635, 0.97247706422018354, 0.98970133882595268, 0.95699075371252451, 0.83333333333333337, 0.94402985074626866, 0.96463993100474343, 0.9668587896253602, 0, 0.66666666666666663]\n"
     ]
    }
   ],
   "source": [
    "print frac"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot\n",
    "plt.close('all')\n",
    "fig, ax = plt.subplots()\n",
    "plt.xlabel('Posted Date', fontsize=12)\n",
    "plt.ylabel('# Positive Fraction', fontsize=12)\n",
    "date_format = mdates.DateFormatter(\"%m/%d/%Y\")\n",
    "ax.xaxis.set_major_formatter(date_format)\n",
    "ax.autoscale_view()\n",
    "width = 0.35\n",
    "plpos = ax.bar(dates, pos_counts, width, color='b', label=\"positive\")\n",
    "plneg = ax.bar(dates, neg_counts, width, color='r', label=\"negative\")\n",
    "#plt.legend()\n",
    "plt.show()\n",
    "plt.close('all')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
