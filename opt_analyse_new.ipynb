{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userName': 'Roxanne Liu', 'recievedDate': '11/08/2015', 'docID': 'ICEB-2015-0002-12058', 'comment': 'It is very enssential for America to keep the international students. They are epquiped with skills and knowledge. The development of America relies on talents. Competition should not be the reason to ask them to leave.', 'postedDate': '11/11/2015'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "comments = []\n",
    "usernames = []\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "# Alternatively, just remove the basic stop words...\n",
    "stop_words = {\"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\"\n",
    "             ,\"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"where\",\"will\",\"with\"}\n",
    "should_stem = 0\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(w):\n",
    "    if(should_stem == 0):\n",
    "        return w\n",
    "    else:\n",
    "        return stemmer.stem(w)\n",
    "    \n",
    "#creating a small test file for convenience\n",
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "x = open(\"opt_dataset/opt.json\")\n",
    "i = 0\n",
    "training = open(\"opt_dataset/small_opt.json\",'w')\n",
    "\n",
    "for l in x:\n",
    "    if (i>=0) and (i<30000):\n",
    "        training.write(l)\n",
    "        if(i==0):\n",
    "            print l\n",
    "    i += 1\n",
    "training.close()\n",
    "\n",
    "# forming the initial vocabulary on the training set of 600 samples\n",
    "# Training set: \n",
    "# Test set: \n",
    "# Not using stemming currently, need to modify later\n",
    "\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sets import Set\n",
    "\n",
    "\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "\n",
    "def bigrams(words):\n",
    "    if(should_stem == 0):\n",
    "        return nltk.bigrams(words)\n",
    "    else:\n",
    "        for i in range(len(words)):\n",
    "            words[i] = stem(words[i])\n",
    "        return nltk.bigrams(words)\n",
    "\n",
    "        \n",
    "# extracing all the known labels\n",
    "labels = []\n",
    "for l in open('opt_dataset/labels_1-700.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "def fit_params(num_examples, data_labels,update_vocab):\n",
    "    train_no = num_examples\n",
    "    # extracting the vocab\n",
    "    if(update_vocab == 1):\n",
    "        vocab_no = train_no\n",
    "    else:\n",
    "        vocab_no = 700\n",
    "        \n",
    "    vocab = Set()\n",
    "    for i in range(vocab_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for w in r.split():\n",
    "            if(w not in stop_words):\n",
    "                w = stem(w)\n",
    "                vocab.add(w)\n",
    "\n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    \n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0.0\n",
    "    numberneg_mult = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            w = stem(w)\n",
    "            if(w in vocab):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult[w] += 1\n",
    "                    numberpos_mult += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "    \n",
    "    return vocab, pos_wordprob_mult,neg_wordprob_mult,posprob, negprob\n",
    "\n",
    "import math\n",
    "def predpos_mult(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "            \n",
    "def predneg_mult(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes bigrams!\n",
    "def fit_params_bi(num_examples, data_labels,update_vocab):\n",
    "    train_no = num_examples\n",
    "    # extracting the bigrams\n",
    "    if(update_vocab == 1):\n",
    "        vocab_no = train_no\n",
    "    else:\n",
    "        vocab_no = 700\n",
    "\n",
    "    all_bi = defaultdict(int)\n",
    "    for i in range(vocab_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for pair in bigrams(r.split()):\n",
    "            all_bi[pair]=1\n",
    "                \n",
    "    pos_wordCount_mult_bi = defaultdict(int)\n",
    "    neg_wordCount_mult_bi = defaultdict(int)\n",
    "    \n",
    "    for pair in all_bi:\n",
    "        pos_wordCount_mult_bi[pair] = 0\n",
    "        neg_wordCount_mult_bi[pair] = 0\n",
    "    numberpos_mult_bi = 0.0\n",
    "    numberneg_mult_bi = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for pair in bigrams(bag):\n",
    "            if(pair in all_bi):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult_bi[pair] += 1\n",
    "                    numberpos_mult_bi += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult_bi[pair] += 1\n",
    "                    numberneg_mult_bi += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult_bi = defaultdict(float)\n",
    "    neg_wordprob_mult_bi = defaultdict(float)\n",
    "\n",
    "    for pair in all_bi:\n",
    "        pos_wordprob_mult_bi[pair] = 1.0*(pos_wordCount_mult_bi[pair] + 1)/(len(all_bi) + numberpos_mult_bi)\n",
    "        neg_wordprob_mult_bi[pair] = 1.0*(neg_wordCount_mult_bi[pair] + 1)/(len(all_bi) + numberneg_mult_bi)\n",
    "    \n",
    "    return all_bi, pos_wordprob_mult_bi,neg_wordprob_mult_bi,posprob, negprob\n",
    "\n",
    "\n",
    "import math\n",
    "def predpos_mult_bi(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for pair in bigrams(words):\n",
    "        if(pair in vocab):\n",
    "            sum += math.log(prob[pair])\n",
    "    return (sum)\n",
    "            \n",
    "def predneg_mult_bi(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for pair in bigrams(words):\n",
    "        if(pair in vocab):\n",
    "            sum += math.log(prob[pair])\n",
    "    return (sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_NB(num_examples,data_labels,update_vocab):\n",
    "    vocab, pp, nn, p, n = fit_params(num_examples,data_labels,update_vocab)\n",
    "    all_bi, pp_bi, nn_bi, p, n = fit_params_bi(num_examples,data_labels,update_vocab)\n",
    "    predictions_mult = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words,vocab,pp,p) #+ predpos_mult_bi(words,all_bi,pp_bi,p)\n",
    "        y = predneg_mult(words,vocab,nn,n) #+ predneg_mult_bi(words,all_bi,nn_bi,n)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    return predictions_mult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# functions for accuracy prediction...\n",
    "def print_accuracies(predictions, no_train,verbatim):\n",
    "    train_accuracy = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    rate = 0.0\n",
    "    neg_examples = 0.0\n",
    "    for i in range(700):\n",
    "        if(i>=no_train and labels[i] == 0):\n",
    "            neg_examples += 1\n",
    "        if(predictions[i] == labels[i]):\n",
    "            if(labels[i] == 0 and i>=no_train):\n",
    "                rate += 1\n",
    "            if i<no_train:\n",
    "                train_accuracy += 1\n",
    "            else:\n",
    "                test_accuracy += 1\n",
    "                \n",
    "        elif(i>=no_train  and i<700 and verbatim == 1):\n",
    "            print comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions[i]\n",
    "    print \"\\n\",\"Test accuracy:\",test_accuracy/(700-no_train),\"Train accuracy:\", train_accuracy/no_train, \"frac negatives identified:\",(rate + 0.0001)/(neg_examples+0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.86 Train accuracy: 0.98 frac negatives identified: 0.333336507921\n"
     ]
    }
   ],
   "source": [
    "predictions = train_NB(600,labels,1)\n",
    "print_accuracies(predictions,600,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\n",
      "\n",
      "Test accuracy: 0.92 Train accuracy: 0.976666666667 frac negatives identified: 0.619049433098\n",
      "iteration number: 2\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.98 frac negatives identified: 0.857143537412\n",
      "iteration number: 3\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.973333333333 frac negatives identified: 0.904762358274\n",
      "iteration number: 4\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.973333333333 frac negatives identified: 0.904762358274\n",
      "iteration number: 5\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n",
      "iteration number: 6\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n",
      "iteration number: 7\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n",
      "iteration number: 8\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n",
      "iteration number: 9\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "# Refer to https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation\n",
    "# running the iterative process a fixed number of times.\n",
    "\n",
    "num_iter = 1\n",
    "\n",
    "while(num_iter < 10):\n",
    "    print \"iteration number:\",num_iter\n",
    "    predictions = train_NB(len(comments),predictions,0)\n",
    "    print_accuracies(predictions,600,0)\n",
    "    num_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I see absolutely no reason for expanding the opportunities F-1 nonimmigrant students who merely make it much harder for United States young people who have such a difficult time finding work. Please do not allow additional visas or additional time for F-1 individuals to work in the U.S. True label: 0 Predicted label: 1\n",
      "While some people arguing that extending OPT will put more Americans out of work, the simple truth is that you cannot ask a farmer to do the job of a painter, and you cannot ask a painter to do the job of an engineer. To be honest, highly educated people with good personality will never have trouble finding a job, regardless of being an American or a foreign student. Those who has trouble finding a job is people who have limited skills or bad personalities. The way to put more Americans at work is to EDUCATE them, not by limiting those who are ALREADY educated. If I am the company manager, if I cannot find the workers with the required skills because you SENT them home, I will likely to move my company to their home country rather than hiring someone here who is incapable, sounds like creating more opportunities for other Americans? what a joke! True label: 1 Predicted label: 0\n",
      "As a US citizen, I personally think OPT program abuse is too prevailing to be acceptable for US public. However, I do believe there should be common ground as US should retain talented foreign students while preventing abuse Therefore I suggest the new OPT rule should be improved by these steps:  1) very clear and high enough prevailing wage requirement for employers to recruit students on OPT. 2) set up a black list of companies that abuse OPT and H1b, those in black list will be forever barred from using OPT or H1b program. 3)OPT program should only be available for students from top 50-100 US universities, the current provision \"accredited Universities\" is too vague. I also suggest Chinese students to comment about how to improve the OPT extension as above, in this case, you can at least win support from relatively reasonable people like me True label: 0 Predicted label: 1\n",
      "\n",
      "Test accuracy: 0.97 Train accuracy: 0.971666666667 frac negatives identified: 0.904762358274\n"
     ]
    }
   ],
   "source": [
    "print_accuracies(predictions,600,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
