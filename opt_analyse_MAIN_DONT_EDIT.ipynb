{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'userName': 'Roxanne Liu', 'recievedDate': '11/08/2015', 'docID': 'ICEB-2015-0002-12058', 'comment': 'It is very enssential for America to keep the international students. They are epquiped with skills and knowledge. The development of America relies on talents. Competition should not be the reason to ask them to leave.', 'postedDate': '11/11/2015'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from sklearn import linear_model\n",
    "from sets import Set\n",
    "\n",
    "comments = []\n",
    "usernames = []\n",
    "#stop_words = set(stopwords.words('english'))\n",
    "# Alternatively, just remove the basic stop words...\n",
    "stop_words = {\"a\",\"an\",\"and\",\"are\",\"as\",\"at\",\"be\",\"by\",\"for\",\"from\",\"has\",\"he\",\"in\",\"is\",\"it\"\n",
    "             ,\"its\",\"of\",\"on\",\"that\",\"the\",\"to\",\"was\",\"where\",\"will\",\"with\"}\n",
    "should_stem = 0\n",
    "punctuation = set(string.punctuation)\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem(w):\n",
    "    if(should_stem == 0):\n",
    "        return w\n",
    "    else:\n",
    "        return stemmer.stem(w)\n",
    "    \n",
    "#creating a small test file for convenience\n",
    "def read(f):\n",
    "    for l in open(f):\n",
    "        yield eval(l)\n",
    "        \n",
    "x = open(\"opt_dataset/opt.json\")\n",
    "i = 0\n",
    "training = open(\"opt_dataset/small_opt.json\",'w')\n",
    "\n",
    "for l in x:\n",
    "    if (i>=0) and (i<20000):\n",
    "        training.write(l)\n",
    "        if(i==0):\n",
    "            print l\n",
    "    i += 1\n",
    "training.close()\n",
    "\n",
    "# forming the initial vocabulary on the training set of 600 samples\n",
    "# Training set: \n",
    "# Test set: \n",
    "# Not using stemming currently, need to modify later\n",
    "\n",
    "\n",
    "for i in read('opt_dataset/small_opt.json'):\n",
    "    comments.append(i['comment'])\n",
    "    usernames.append(i['userName'])\n",
    "\n",
    "def bigrams(words):\n",
    "    if(should_stem == 0):\n",
    "        return nltk.bigrams(words)\n",
    "    else:\n",
    "        for i in range(len(words)):\n",
    "            words[i] = stem(words[i])\n",
    "        return nltk.bigrams(words)\n",
    "\n",
    "        \n",
    "# extracing all the known labels\n",
    "labels = []\n",
    "for l in open('opt_dataset/labels_1-900.txt'):\n",
    "    if(eval(l) == 1):\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes!\n",
    "def fit_params(num_examples, data_labels,update_vocab):\n",
    "    train_no = num_examples\n",
    "    # extracting the vocab\n",
    "    if(update_vocab == 1):\n",
    "        vocab_no = train_no\n",
    "    else:\n",
    "        vocab_no = 700\n",
    "        \n",
    "    vocab = Set()\n",
    "    for i in range(vocab_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for w in r.split():\n",
    "            if(w not in stop_words):\n",
    "                w = stem(w)\n",
    "                vocab.add(w)\n",
    "\n",
    "    pos_wordCount_mult = defaultdict(int)\n",
    "    neg_wordCount_mult = defaultdict(int)\n",
    "    \n",
    "    for w in vocab:\n",
    "        pos_wordCount_mult[w] = 0\n",
    "        neg_wordCount_mult[w] = 0\n",
    "    numberpos_mult = 0.0\n",
    "    numberneg_mult = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for w in bag:\n",
    "            w = stem(w)\n",
    "            if(w in vocab):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult[w] += 1\n",
    "                    numberpos_mult += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult[w] += 1\n",
    "                    numberneg_mult += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult = defaultdict(float)\n",
    "    neg_wordprob_mult = defaultdict(float)\n",
    "\n",
    "    for w in vocab:\n",
    "        pos_wordprob_mult[w] = 1.0*(pos_wordCount_mult[w] + 1)/(len(vocab) + numberpos_mult)\n",
    "        neg_wordprob_mult[w] = 1.0*(neg_wordCount_mult[w] + 1)/(len(vocab) + numberneg_mult)\n",
    "    \n",
    "    return vocab, pos_wordprob_mult,neg_wordprob_mult,posprob, negprob\n",
    "\n",
    "import math\n",
    "def predpos_mult(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "            \n",
    "def predneg_mult(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for w in words:\n",
    "        w = stem(w)\n",
    "        if(w in vocab):\n",
    "            sum += math.log(prob[w])\n",
    "    return (math.log(frac) + sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Multinomial naive bayes bigrams!\n",
    "def fit_params_bi(num_examples, data_labels,update_vocab):\n",
    "    train_no = num_examples\n",
    "    # extracting the bigrams\n",
    "    if(update_vocab == 1):\n",
    "        vocab_no = train_no\n",
    "    else:\n",
    "        vocab_no = 700\n",
    "\n",
    "    all_bi = defaultdict(int)\n",
    "    for i in range(vocab_no):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        for pair in bigrams(r.split()):\n",
    "            all_bi[pair]=1\n",
    "                \n",
    "    pos_wordCount_mult_bi = defaultdict(int)\n",
    "    neg_wordCount_mult_bi = defaultdict(int)\n",
    "    \n",
    "    for pair in all_bi:\n",
    "        pos_wordCount_mult_bi[pair] = 0\n",
    "        neg_wordCount_mult_bi[pair] = 0\n",
    "    numberpos_mult_bi = 0.0\n",
    "    numberneg_mult_bi = 0.0\n",
    "    posexamples = 0.0\n",
    "    negexamples = 0.0\n",
    "    # calculating the probabilites\n",
    "    for i in range(train_no):\n",
    "        if(data_labels[i] == 1):\n",
    "            posexamples += 1\n",
    "        else:\n",
    "            negexamples += 1\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        bag = r.split()\n",
    "        for pair in bigrams(bag):\n",
    "            if(pair in all_bi):\n",
    "                if(data_labels[i] == 1):\n",
    "                    pos_wordCount_mult_bi[pair] += 1\n",
    "                    numberpos_mult_bi += 1\n",
    "                else:\n",
    "                    neg_wordCount_mult_bi[pair] += 1\n",
    "                    numberneg_mult_bi += 1\n",
    "                    \n",
    "    posprob = posexamples/(posexamples + negexamples)\n",
    "    negprob = negexamples/(posexamples + negexamples)\n",
    "    \n",
    "    pos_wordprob_mult_bi = defaultdict(float)\n",
    "    neg_wordprob_mult_bi = defaultdict(float)\n",
    "\n",
    "    for pair in all_bi:\n",
    "        pos_wordprob_mult_bi[pair] = 1.0*(pos_wordCount_mult_bi[pair] + 1)/(len(all_bi) + numberpos_mult_bi)\n",
    "        neg_wordprob_mult_bi[pair] = 1.0*(neg_wordCount_mult_bi[pair] + 1)/(len(all_bi) + numberneg_mult_bi)\n",
    "    \n",
    "    return all_bi, pos_wordprob_mult_bi,neg_wordprob_mult_bi,posprob, negprob\n",
    "\n",
    "\n",
    "import math\n",
    "def predpos_mult_bi(words,vocab,prob, frac):\n",
    "    sum = 0.0\n",
    "    for pair in bigrams(words):\n",
    "        if(pair in vocab):\n",
    "            sum += math.log(prob[pair])\n",
    "    return (sum)\n",
    "            \n",
    "def predneg_mult_bi(words, vocab, prob, frac):\n",
    "    sum = 0.0\n",
    "    for pair in bigrams(words):\n",
    "        if(pair in vocab):\n",
    "            sum += math.log(prob[pair])\n",
    "    return (sum)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_NB(num_examples,data_labels,update_vocab):\n",
    "    vocab, pp, nn, p, n = fit_params(num_examples,data_labels,update_vocab)\n",
    "    all_bi, pp_bi, nn_bi, p, n = fit_params_bi(num_examples,data_labels,update_vocab)\n",
    "    predictions_mult = []\n",
    "    for i in range(len(comments)):\n",
    "        r = ''.join([c for c in comments[i].lower() if not c in punctuation])\n",
    "        words = r.split()\n",
    "        x = predpos_mult(words,vocab,pp,p) #+ predpos_mult_bi(words,all_bi,pp_bi,p)\n",
    "        y = predneg_mult(words,vocab,nn,n) #+ predneg_mult_bi(words,all_bi,nn_bi,n)\n",
    "        if(x > y):\n",
    "            predictions_mult.append(1)\n",
    "        else:\n",
    "            predictions_mult.append(0)\n",
    "    return predictions_mult\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# functions for accuracy prediction...\n",
    "def print_accuracies(predictions, no_train,verbatim):\n",
    "    train_accuracy = 0.0\n",
    "    test_accuracy = 0.0\n",
    "    valid_accuracy = 0.0\n",
    "    tnrate = 0.0\n",
    "    tprate = 0.0\n",
    "    neg_examples = 0.0\n",
    "    for i in range(900):\n",
    "        if(i>=700 and labels[i] == 0):\n",
    "            neg_examples += 1\n",
    "        if(predictions[i] == labels[i]):\n",
    "            if(labels[i] == 0 and i>=700):\n",
    "                tnrate += 1\n",
    "            if(labels[i] == 1 and i>=700):\n",
    "                tprate += 1\n",
    "            if i<600:\n",
    "                train_accuracy += 1\n",
    "            elif i<700:\n",
    "                valid_accuracy += 1\n",
    "            else:\n",
    "                test_accuracy += 1\n",
    "                \n",
    "        elif(i>=700  and i<900 and verbatim == 1):\n",
    "            print \"\\n\", \"comment #:\",i,comments[i], \"True label:\",labels[i],\"Predicted label:\",predictions[i]\n",
    "    print \"\\n\", \"Train accuracy:\", train_accuracy/no_train, \"Validation accuracy:\", valid_accuracy/(100),\\\n",
    "    \"Test accuracy:\",test_accuracy/(200),\"TNR:\",(tnrate + 0.0001)/(neg_examples+0.0001), \"TPR:\", (tprate + 0.0001)/(200-neg_examples+0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train accuracy: 0.98 Validationa accuracy: 0.86 Test accuracy: 0.905 True negative rate: 0.407409602187\n"
     ]
    }
   ],
   "source": [
    "predictions = train_NB(600,labels,1)\n",
    "print_accuracies(predictions,600,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number: 1\n",
      "\n",
      "Train accuracy: 0.98 Validationa accuracy: 0.92 Test accuracy: 0.94 True negative rate: 0.703704801093\n",
      "iteration number: 2\n",
      "\n",
      "Train accuracy: 0.981666666667 Validationa accuracy: 0.96 Test accuracy: 0.96 True negative rate: 0.88888930041\n",
      "iteration number: 3\n",
      "\n",
      "Train accuracy: 0.975 Validationa accuracy: 0.97 Test accuracy: 0.965 True negative rate: 0.925926200273\n",
      "iteration number: 4\n",
      "\n",
      "Train accuracy: 0.975 Validationa accuracy: 0.97 Test accuracy: 0.965 True negative rate: 0.925926200273\n",
      "iteration number: 5\n",
      "\n",
      "Train accuracy: 0.975 Validationa accuracy: 0.97 Test accuracy: 0.965 True negative rate: 0.925926200273\n",
      "iteration number: 6\n",
      "\n",
      "Train accuracy: 0.975 Validationa accuracy: 0.97 Test accuracy: 0.965 True negative rate: 0.925926200273\n",
      "iteration number: 7\n",
      "\n",
      "Train accuracy: 0.973333333333 Validationa accuracy: 0.97 Test accuracy: 0.965 True negative rate: 0.925926200273\n",
      "iteration number: 8\n",
      "\n",
      "Train accuracy: 0.973333333333 Validationa accuracy: 0.97 Test accuracy: 0.965 True negative rate: 0.925926200273\n",
      "iteration number: 9\n",
      "\n",
      "Train accuracy: 0.973333333333 Validationa accuracy: 0.97 Test accuracy: 0.96 True negative rate: 0.925926200273\n"
     ]
    }
   ],
   "source": [
    "# Semi supervised learning, Some kind of expectation maximization...\n",
    "# Refer to https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Semi-supervised_parameter_estimation\n",
    "# running the iterative process a fixed number of times.\n",
    "\n",
    "num_iter = 1\n",
    "\n",
    "while(num_iter < 10):\n",
    "    print \"iteration number:\",num_iter\n",
    "    predictions = train_NB(len(comments),predictions,0)\n",
    "    print_accuracies(predictions,600,0)\n",
    "    num_iter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "comment #: 718 OPT is helping to find better workers for the jobs, not simply give the jobs to foreigners. True label: 1 Predicted label: 0\n",
      "\n",
      "comment #: 765 Everyone should be allowed a green card so they can start paying taxes and be tracked True label: 1 Predicted label: 0\n",
      "\n",
      "comment #: 806 The Optional Practical Training program allows U.S. employers to hire foreign students for up to 1 year or up to 29 months in certain fields of study. The program was meant to provide foreign students with on-the-job . True label: 1 Predicted label: 0\n",
      "\n",
      "comment #: 818 I oppose the extension of OPT. Schools, especially public schools, welcome foreign students because they pay high tuition. And then, with extension of OPT, they earn back what they invest and maybe much more. Who is the winner? Obviously, foreign students who get more than they invested, schools which get a lot of money and companies which get a lot of comparable cheap workers. Who is the loser? Obviously not the government, the working Americans are loser, the middle class is loser. They take risk of losing their jobs but they don't get any benefit from having more and more foreign students. True label: 0 Predicted label: 1\n",
      "\n",
      "comment #: 838 I do NOT think extending the OPT for STEM major is a fair and right decision.  First of all, this is way too unfair for non-STEM major students, who accounts for a large source of tuition income for universities and colleges. If the extension passes, it means that STEM students might have more chances to get the lotteries, so that the ones who didn't get an H1B quota will roll over to next year, and as a result, the chances of non-STEM majors could win a lottery is even lower.  Secondly, extending OPT for STEM would cause more problem to the society as a whole. Extending OPT itself is okay, but a direct result of this is that more STEM students will be able to get a H1B and in turn a green card later. This will not only cause series problems due to immigration, but also will make less American willing to study STEM majors and make the relevant industries dominated by aliens.  To sum it up, extending OPT for STEM is NEVER gonna be a right decision! True label: 0 Predicted label: 1\n",
      "\n",
      "comment #: 892 Admittedly, there are Americans who can not find a job. But there are also foreign students who can not find a job. The majority of US companies already give priorities to US workers. As a result, the unemployment percentage of international students is already higher than that of native Americans. It is unfair to say that more US workers can not find a job. We should compare the percentage instead of the absolute number. True label: 1 Predicted label: 0\n",
      "\n",
      "Train accuracy: 0.973333333333 Validation accuracy: 0.97 Test accuracy: 0.97 TNR: 0.93103472057 TPR: 0.976608200814\n"
     ]
    }
   ],
   "source": [
    "print_accuracies(predictions,600,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
